import pandas as pd
import requests
import json
from datetime import datetime, UTC, timedelta
import logging
import time
from typing import Dict, List, Tuple, Any, Optional
import os # Need os for file operations
import tempfile # For creating temporary files safely

from ratelimit import limits, sleep_and_retry
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest, NotFound
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration ---
GCP_PROJECT_ID = "silver-455021"  # <-- REPLACE THIS
GCP_LOCATION = "us-central1"         # Region for Vertex AI and BQ connection
BQ_LOCATION = "US"                   # Location for BigQuery dataset
BQ_DATASET_ID = "mlb_rag_data_2024"  # Dataset to store RAG documents
BQ_RAG_TABLE_ID = "rag_documents"    # Table for documents, embeddings, metadata
BQ_INDEX_NAME = "rag_docs_embedding_idx" # Name for the BQ vector index

# Vertex AI Models
VERTEX_LLM_MODEL = "gemini-2.0-flash-lite" # Or gemini-1.0-pro, etc.
VERTEX_EMB_MODEL = "text-embedding-005"
EMBEDDING_TASK_TYPE = "RETRIEVAL_DOCUMENT" # Task type for embeddings
EMBEDDING_DIMENSIONALITY = 768 # Dimension for text-embedding-005

# MLB API Rate Limiting
MLB_API_CALLS = 9 # Be conservative, official limit is 10/min
MLB_API_RATE_LIMIT = 60 # seconds

# Vertex AI API Quotas (Adjust based on your project's quotas)
# Find quotas in GCP Console -> IAM & Admin -> Quotas -> Filter by Vertex AI API
VERTEX_LLM_RPM = 180 # Requests per minute (example for gemini-1.5-flash)
VERTEX_EMB_RPM = 1400 # Requests per minute (example for text-embedding-005)

# How many recent games to process per team
NUM_GAMES_PER_TEAM = 10 # Set low for testing, increase for production

# Team configurations (shortened for brevity)
TEAMS = {
    'rangers': 140,
    'angels': 108,
    'astros': 117,
    'rays': 139,
    'blue_jays': 141,
    'yankees': 147,
    'orioles': 110,
    'red_sox': 111,
    'twins': 142,
    'white_sox': 145,
    'guardians': 114,
    'tigers': 116,
    'royals': 118,
    'padres': 135,
    'giants': 137,
    'diamondbacks': 109,
    'rockies': 115,
    'phillies': 143,
    'braves': 144,
    'marlins': 146,
    'nationals': 120,
    'mets': 121,
    'pirates': 134,
    'cardinals': 138,
    'brewers': 158,
    'cubs': 112,
    'reds': 113,
    'athletics': 133,
    'mariners': 136,
    'dodgers': 119,
}

# --- BigQuery Client and Vertex AI Initialization ---
try:
    bq_client = bigquery.Client(project=GCP_PROJECT_ID)
    vertexai.init(project=GCP_PROJECT_ID, location=GCP_LOCATION)
    # Load models (do this once)
    llm_model = GenerativeModel(VERTEX_LLM_MODEL)
    emb_model = TextEmbeddingModel.from_pretrained(VERTEX_EMB_MODEL)
    logger.info(f"Initialized BigQuery client and Vertex AI SDK for project {GCP_PROJECT_ID}")
except Exception as e:
    logger.error(f"Failed to initialize Google Cloud clients: {e}")
    exit() # Critical failure

# --- RAG Table Schema ---
RAG_SCHEMA = [
    bigquery.SchemaField("doc_id", "STRING", mode="REQUIRED", description="Unique ID for the document (e.g., game_pk_summary, game_pk_play_idx)"),
    bigquery.SchemaField("game_id", "INTEGER", mode="REQUIRED", description="MLB gamePk"),
    bigquery.SchemaField("doc_type", "STRING", mode="NULLABLE", description="Type of document (e.g., 'game_summary', 'play_detail')"),
    bigquery.SchemaField("content", "STRING", mode="NULLABLE", description="Natural language text content generated by LLM"),
    bigquery.SchemaField("embedding", "FLOAT64", mode="REPEATED", description=f"Vector embedding ({EMBEDDING_DIMENSIONALITY} dimensions)"),
    bigquery.SchemaField("metadata", "JSON", mode="NULLABLE", description="Structured metadata associated with the content"),
    bigquery.SchemaField("last_updated", "TIMESTAMP", mode="NULLABLE"),
]

# --- Utility Functions ---
@sleep_and_retry
@limits(calls=MLB_API_CALLS, period=MLB_API_RATE_LIMIT)
def call_mlb_api(url: str) -> Dict:
    """Make a rate-limited call to the MLB API"""
    try:
        response = requests.get(url, timeout=20) # Add timeout
        response.raise_for_status()
        # Check if response is valid JSON
        if 'application/json' in response.headers.get('Content-Type', ''):
            return response.json()
        else:
            logger.warning(f"Non-JSON response received from MLB API: {url}. Status: {response.status_code}. Content: {response.text[:100]}...")
            return {} # Return empty dict to signal issue
    except requests.exceptions.RequestException as e:
        logger.error(f"Error calling MLB API {url}: {e}")
        return {} # Return empty dict on error

def ensure_dataset_exists(client: bigquery.Client, dataset_id: str):
    """Create BQ dataset if it doesn't exist"""
    full_dataset_id = f"{client.project}.{dataset_id}"
    try:
        client.get_dataset(full_dataset_id)
        logger.info(f"Dataset {full_dataset_id} already exists")
    except NotFound:
        dataset = bigquery.Dataset(full_dataset_id)
        dataset.location = BQ_LOCATION
        dataset = client.create_dataset(dataset, timeout=30)
        logger.info(f"Created dataset {full_dataset_id}")
    except Exception as e:
        logger.error(f"Error checking/creating dataset {full_dataset_id}: {e}")
        raise

def create_rag_table(client: bigquery.Client, dataset_id: str, table_id: str):
    """Create the RAG table in BigQuery if it doesn't exist."""
    full_table_id = f"{client.project}.{dataset_id}.{table_id}"
    try:
        client.get_table(full_table_id)
        logger.info(f"Table {full_table_id} already exists.")
    except NotFound:
        logger.info(f"Table {full_table_id} not found, creating...")
        table = bigquery.Table(full_table_id, schema=RAG_SCHEMA)
        try:
            client.create_table(table)
            logger.info(f"Created table {full_table_id}")
        except Exception as e:
            logger.error(f"Failed to create table {full_table_id}: {e}")
            raise
    except Exception as e:
         logger.error(f"Error checking table {full_table_id}: {e}")
         raise

# Rate limiter for Vertex AI LLM calls
@sleep_and_retry
@limits(calls=VERTEX_LLM_RPM, period=60)
def call_vertex_llm(prompt: str) -> Optional[str]:
    """Make a rate-limited call to the Vertex AI LLM."""
    try:
        # Configure Gemini generation
        generation_config = GenerationConfig(
            temperature=0.2, # More factual
            max_output_tokens=1024
        )
        response = llm_model.generate_content(
            prompt,
            generation_config=generation_config,
            # safety_settings=... # Add safety settings if needed
            )
        # Basic check for response content
        if response.candidates and response.candidates[0].content.parts:
             # Handle potential lack of text part (though unlikely for text prompts)
            text_parts = [part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')]
            if text_parts:
                return "\n".join(text_parts).strip()
            else:
                 logger.warning(f"LLM response received but no text part found for prompt: {prompt[:100]}...")
                 return None
        else:
            # Log the blocking reason if available
            block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else 'Unknown'
            logger.warning(f"LLM generation blocked or empty. Reason: {block_reason}. Prompt: {prompt[:100]}...")
            return None
    except Exception as e:
        logger.error(f"Error calling Vertex AI LLM: {e}. Prompt: {prompt[:100]}...")
        return None

# Rate limiter for Vertex AI Embedding calls
# Note: The SDK handles batching internally to some extent, but explicit batching + rate limiting is safer for large loads
@sleep_and_retry
@limits(calls=VERTEX_EMB_RPM, period=60)
def call_vertex_embedding(text_inputs: List[str]) -> List[Optional[List[float]]]:
    """Make a rate-limited call to the Vertex AI Embedding API."""
    results = []
    try:
        # Max 250 inputs per API call for text-embedding-005
        instances = [TextEmbeddingInput(text=text, task_type=EMBEDDING_TASK_TYPE) for text in text_inputs]
        kwargs = {"output_dimensionality": EMBEDDING_DIMENSIONALITY} if EMBEDDING_TASK_TYPE != "RETRIEVAL_DOCUMENT" else {}

        embeddings = emb_model.get_embeddings(instances, **kwargs)
        results = [emb.values for emb in embeddings]
        # Basic validation
        if not all(len(emb) == EMBEDDING_DIMENSIONALITY for emb in results if emb):
             logger.warning(f"Generated embeddings have unexpected dimensionality. Expected {EMBEDDING_DIMENSIONALITY}.")
             # Handle or filter out malformed embeddings if needed

        return results # Returns list of lists (embeddings)

    except Exception as e:
        logger.error(f"Error calling Vertex AI Embedding API for batch size {len(text_inputs)}: {e}")
        # Return None for each input in the failed batch
        return [None] * len(text_inputs)




def upload_to_bigquery(df: pd.DataFrame, table_id: str, schema: List[bigquery.SchemaField]):
    """Upload DataFrame to BigQuery table via a temporary NDJSON file."""
    if df.empty:
        logger.info(f"DataFrame is empty, skipping upload to {table_id}.")
        return

    # 1. Prepare data: Ensure metadata is dict/list for json.dumps, timestamp is correct
    if 'metadata' in df.columns:
        # Ensure it's serializable - apply dumps only during file write
        pass # No need to pre-convert to string here for to_json

    if 'last_updated' in df.columns:
         df['last_updated'] = pd.to_datetime(df['last_updated'], utc=True)
         # Convert Timestamp to ISO format string suitable for BQ TIMESTAMP
         # BQ Load job handles ISO format string -> TIMESTAMP conversion
         df['last_updated'] = df['last_updated'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')

    # 2. Use a temporary file
    # Use NamedTemporaryFile to ensure it gets cleaned up even on error
    with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.ndjson') as temp_f:
        temp_file_path = temp_f.name
        logger.info(f"Writing DataFrame to temporary NDJSON file: {temp_file_path}")

        # 3. Write DataFrame to newline-delimited JSON format
        # orient='records' gives one JSON object per line
        # lines=True ensures newline delimited
        # date_format='iso' handles timestamp serialization correctly
        df.to_json(
            temp_f,
            orient='records',
            lines=True,
            date_format='iso', # Important for timestamps
            force_ascii=False # Handle potential non-ASCII chars in summaries
        )
        # Ensure data is written before BQ reads it
        temp_f.flush()

    logger.info(f"Finished writing to temporary file. Starting BigQuery load from file.")

    # 4. Configure and run the BigQuery load job from the file
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_APPEND",
        schema=schema,
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, # Specify NDJSON format
        # BQ can auto-detect schema usually, but specifying ensures consistency
        # autodetect=False # Explicitly use provided schema
    )

    try:
        with open(temp_file_path, "rb") as source_file: # Open in binary mode for upload
            job = bq_client.load_table_from_file(source_file, table_id, job_config=job_config)

        job.result()  # Wait for the job to complete
        table = bq_client.get_table(table_id) # Refresh table info
        logger.info(
            f"Loaded {job.output_rows} rows to {table_id}. Total rows: {table.num_rows}. Job ID: {job.job_id}"
        )

    except BadRequest as e:
        logger.error(f"BigQuery BadRequest loading from file {temp_file_path} to {table_id}: {e}")
        if hasattr(e, 'errors'):
            for error in e.errors:
                logger.error(f"  Reason: {error.get('reason', 'N/A')}, Location: {error.get('location', 'N/A')}, Message: {error.get('message', 'N/A')}")
        # Keep the temp file for debugging if needed, otherwise it gets deleted below
        # raise

    except Exception as e:
        logger.error(f"Generic error loading data from file {temp_file_path} to {table_id}: {e}")
        # Keep the temp file for debugging if needed, otherwise it gets deleted below
        # raise
    finally:
         # 5. Clean up the temporary file
        try:
            os.remove(temp_file_path)
            logger.info(f"Removed temporary file: {temp_file_path}")
        except OSError as e:
            logger.error(f"Error removing temporary file {temp_file_path}: {e}")

# --- Core Processing Functions ---

def get_recent_game_ids(team_id: int, season: int = 2024, num_games: int = 10) -> List[int]:
    """Fetch recent game IDs for a specified team and season"""
    url = f'https://statsapi.mlb.com/api/v1/schedule?sportId=1&season={season}&teamId={team_id}&fields=dates,games,gamePk,officialDate,status,detailedState'
    schedule_data = call_mlb_api(url)
    game_ids = []
    if schedule_data and 'dates' in schedule_data:
        all_games = []
        for date_entry in schedule_data.get('dates', []):
            for game in date_entry.get('games', []):
                 # Only consider final games for this RAG pipeline example
                 if game.get('status', {}).get('detailedState') == 'Final':
                    all_games.append({
                        'game_id': game.get('gamePk'),
                        'date': game.get('officialDate')
                    })

        # Sort by date descending and take the most recent N
        all_games.sort(key=lambda x: x['date'], reverse=True)
        game_ids = [game['game_id'] for game in all_games[:num_games] if game['game_id']]

    if not game_ids:
         logger.warning(f"No recent final game IDs found for team {team_id}, season {season}.")

    return game_ids


def get_full_game_data(game_pk: int) -> Dict:
    """Fetch the full live feed data for a specific game."""
    url = f'https://statsapi.mlb.com/api/v1.1/game/{game_pk}/feed/live'
    logger.info(f"Fetching full data for game {game_pk}")
    data = call_mlb_api(url)
    if not data:
        logger.error(f"Failed to fetch or received empty data for game {game_pk}")
    return data
def generate_game_summary_and_metadata(game_pk: int, game_data: Dict) -> Optional[Tuple[str, Dict]]:
    """Generates a natural language summary and structured metadata for a game using an LLM."""
    # Initial check for essential top-level keys
    if not game_data or 'gameData' not in game_data or 'liveData' not in game_data:
        logger.warning(f"generate_game_summary_and_metadata: Invalid or incomplete game_data for game {game_pk}.")
        return None

    try:
        # --- Corrected Key Access ---
        game_info = game_data.get('gameData', {})
        live_info = game_data.get('liveData', {})

        # Extract gamePk safely (using the passed-in game_pk as fallback if needed)
        actual_game_pk = game_info.get('game', {}).get('pk', game_pk)
        if actual_game_pk != game_pk:
             logger.warning(f"Mismatch between passed game_pk ({game_pk}) and pk in data ({actual_game_pk}). Using pk from data.")
             game_pk = actual_game_pk # Use the one from the data if available

        status = game_info.get('status', {}).get('detailedState')

        if status != 'Final':
            logger.info(f"Game {game_pk} is not Final ({status}), skipping summary generation.")
            return None

        # --- Use .get() for safer access throughout ---
        home_team_data = game_info.get('teams', {}).get('home', {})
        away_team_data = game_info.get('teams', {}).get('away', {})
        home_team = home_team_data.get('name', 'Unknown Home Team')
        away_team = away_team_data.get('name', 'Unknown Away Team')
        home_id = home_team_data.get('id')
        away_id = away_team_data.get('id')
        date = game_info.get('datetime', {}).get('officialDate')
        venue = game_info.get('venue', {}).get('name', 'Unknown Venue')

        linescore = live_info.get('linescore', {})
        home_score = linescore.get('teams', {}).get('home', {}).get('runs', 0)
        away_score = linescore.get('teams', {}).get('away', {}).get('runs', 0)
        innings_data = linescore.get('innings', [])

        # Basic play info extraction
        key_plays_info = []
        all_plays = live_info.get('plays', {}).get('allPlays', [])
        # scoring_plays_indices = live_info.get('plays', {}).get('scoringPlays', []) # API v1.1 doesn't seem to have this index directly
        
        # Find scoring plays manually
        scoring_plays = [play for play in all_plays if play.get('about', {}).get('isScoringPlay', False)]

        for play in scoring_plays:
             play_result = play.get('result', {})
             play_about = play.get('about', {})
             if play_result and play_about:
                 key_plays_info.append(f"Inning {play_about.get('inning', '?')} ({play_about.get('halfInning', '')}): {play_result.get('description', 'N/A')}")
        key_plays_str = "\n".join(key_plays_info) if key_plays_info else "No specific scoring plays highlighted."

        # Construct metadata (ensure values are serializable, handle potential None for IDs)
        metadata = {
            "date": date,
            "season": int(date[:4]) if date else None,
            "home_team_id": int(home_id) if home_id else None,
            "home_team_name": home_team,
            "away_team_id": int(away_id) if away_id else None,
            "away_team_name": away_team,
            "home_score": int(home_score),
            "away_score": int(away_score),
            "venue_name": venue,
            "status": status,
            "innings": len(innings_data),
            # Add more metadata as needed (e.g., from boxscore if fetched)
            # "winning_pitcher_id": ...,
            # "losing_pitcher_id": ...,
            # "key_player_stats": [...]
        }

        # Construct Prompt
        prompt = f"""
        Analyze the following MLB game data and provide a concise summary suitable for answering questions about the game.

        Game Details:
        - Date: {date}
        - Teams: {away_team} (Away) vs. {home_team} (Home)
        - Final Score: Away {away_score}, Home {home_score}
        - Venue: {venue}
        - Number of Innings: {len(innings_data)}

        Key Scoring Plays:
        {key_plays_str}

        Instructions:
        Summarize the game's outcome, mention the final score, the teams involved, and the date. Briefly mention the flow of scoring if possible based on the key plays. Do not list every play. Focus on the overall result and significant scoring events. Keep the summary to 2-4 sentences.
        """

        logger.info(f"Generating summary for game {game_pk}")
        summary = call_vertex_llm(prompt)

        if summary:
            return summary, metadata
        else:
            logger.warning(f"LLM failed to generate summary for game {game_pk}")
            return None

    except KeyError as e:
         # Log with the game_pk that was passed in
         logger.error(f"KeyError processing game data for summary generation (game {game_pk}). Missing key: {e}", exc_info=True)
         return None
    except Exception as e:
        # Log with the game_pk that was passed in
        logger.error(f"Unexpected error generating summary for game {game_pk}: {e}", exc_info=True)
        return None

def create_bq_vector_index(client: bigquery.Client, dataset_id: str, table_id: str, index_name: str):
    """Creates a vector index on the embedding column if it doesn't exist."""
    full_table_id = f"{client.project}.{dataset_id}.{table_id}"
    index_check_sql = f"""
    SELECT index_name
    FROM `{client.project}.{dataset_id}`.INFORMATION_SCHEMA.VECTOR_INDEXES
    WHERE table_name = '{table_id}' AND index_name = '{index_name}';
    """
    create_index_sql = f"""
    CREATE OR REPLACE VECTOR INDEX `{index_name}`
    ON `{full_table_id}`(embedding)
    OPTIONS(distance_type='COSINE', index_type='IVF');
    """ # IVF is often a good default, consider TREE_AH for lower latency if needed

    try:
        # Check if index exists
        query_job = client.query(index_check_sql)
        results = query_job.result()
        if results.total_rows > 0:
            logger.info(f"Vector index {index_name} already exists on {full_table_id}.")
            return

        # Create index if it doesn't exist
        logger.info(f"Creating vector index {index_name} on {full_table_id}...")
        query_job = client.query(create_index_sql)
        query_job.result() # Wait for creation to start (indexing happens async)
        logger.info(f"Vector index {index_name} creation initiated. Indexing will proceed in the background.")

    except Exception as e:
        logger.error(f"Error checking or creating vector index {index_name}: {e}")

# --- Main Processing Logic ---

def process_game_for_rag(game_pk: int, full_table_id: str):
    """Fetches, summarizes, embeds, and uploads data for a single game."""
    game_data = get_full_game_data(game_pk)
    if not game_data:
        logger.warning(f"Skipping game {game_pk} due to fetch failure or empty data.")
        return # Skip if data fetching failed

    # --- Pass game_pk to the summary function ---
    summary_result = generate_game_summary_and_metadata(game_pk, game_data)
    # -----------------------------------------

    if not summary_result:
        logger.warning(f"Skipping game {game_pk} because summary/metadata generation failed.")
        return # Skip if summary generation failed

    summary_text, metadata = summary_result

    # Generate embedding for the summary
    embedding_list = call_vertex_embedding([summary_text])
    if not embedding_list or embedding_list[0] is None:
        logger.error(f"Failed to generate embedding for game {game_pk}. Skipping upload for this game.")
        return

    embedding = embedding_list[0]

    # Prepare data for BigQuery
    doc_id = f"game_{game_pk}_summary"
    rag_data = {
        "doc_id": [doc_id],
        "game_id": [game_pk],
        "doc_type": ["game_summary"],
        "content": [summary_text],
        "embedding": [embedding],
        "metadata": [metadata], # BQ expects dict for JSON col load from DF
        "last_updated": [datetime.now(UTC)]
    }
    rag_df = pd.DataFrame(rag_data)

    # Upload to BigQuery
    logger.info(f"Attempting to upload summary for game {game_pk} to {full_table_id}")
    upload_to_bigquery(rag_df, full_table_id, RAG_SCHEMA)

def main():
    """Main function to process data for all configured teams."""
    start_time = time.time()
    logger.info("Starting MLB RAG data pipeline...")

    dataset_id = BQ_DATASET_ID
    table_id = BQ_RAG_TABLE_ID
    full_table_id = f"{GCP_PROJECT_ID}.{dataset_id}.{table_id}"

    # Ensure Dataset and Table exist before processing teams
    try:
        ensure_dataset_exists(bq_client, dataset_id)
        create_rag_table(bq_client, dataset_id, table_id)
    except Exception as e:
        logger.critical(f"Failed to ensure BQ dataset/table exists. Exiting. Error: {e}")
        return # Stop execution if basic setup fails

    all_game_pks_to_process = set()

    # 1. Gather all unique recent game IDs first
    logger.info("Gathering recent game IDs for all teams...")
    for team_name, team_id in TEAMS.items():
        logger.info(f"Fetching game IDs for {team_name}...")
        try:
            team_game_ids = get_recent_game_ids(team_id, num_games=NUM_GAMES_PER_TEAM)
            if team_game_ids:
                all_game_pks_to_process.update(team_game_ids)
            logger.info(f"Found {len(team_game_ids)} recent game(s) for {team_name}.")
            time.sleep(1) # Small delay between teams
        except Exception as e:
            logger.error(f"Error fetching game IDs for {team_name} (ID: {team_id}): {e}")

    logger.info(f"Total unique recent games to process: {len(all_game_pks_to_process)}")

    # 2. Process each unique game
    processed_count = 0
    for game_pk in all_game_pks_to_process:
        try:
            logger.info(f"--- Processing Game PK: {game_pk} ---")
            process_game_for_rag(game_pk, full_table_id)
            processed_count += 1
            # Add a small delay to avoid hitting lower-level API limits if any exist
            time.sleep(0.5)
        except Exception as e:
            logger.error(f"Critical error processing game {game_pk}: {e}", exc_info=True)
            # Decide whether to continue or stop on critical errors
            # continue

    # 3. Create Vector Index (after all data is loaded)
    logger.info("Data loading complete. Ensuring vector index exists...")
    create_bq_vector_index(bq_client, dataset_id, table_id, BQ_INDEX_NAME)

    end_time = time.time()
    logger.info(f"MLB RAG data pipeline finished. Processed {processed_count} games.")
    logger.info(f"Total execution time: {end_time - start_time:.2f} seconds")

if __name__ == "__main__":
    # --- IMPORTANT SETUP ---
    # 1. Replace '[your-gcp-project-id]' at the top.
    # 2. Ensure you have authenticated (`gcloud auth application-default login`).
    # 3. Ensure necessary APIs are enabled in your GCP project.
    # 4. Ensure necessary Python libraries are installed.
    # 5. Review and adjust rate limits and model names if needed.
    # --- --- --- --- --- ---
    main()



#This script provides a solid foundation. You can enhance it further by:

#Adding more detailed parsing for player stats to include in metadata.

#Generating embeddings for individual key plays (creating more documents per game).

#Implementing more robust error handling and retries.

#Using asynchronous processing or threading for faster API calls (like Tutorial 1).

#Adding a mechanism to track already processed games to avoid redundant work on subsequent runs.

#Improving prompt engineering for better summaries.