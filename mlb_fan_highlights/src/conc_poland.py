import pandas as pd
import requests
import json
from datetime import datetime, UTC, timedelta
import logging
import time
from typing import Dict, List, Tuple, Any, Optional

from ratelimit import limits, sleep_and_retry
from google.cloud import bigquery
from google.api_core.exceptions import BadRequest, NotFound
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Configuration ---
GCP_PROJECT_ID = "[your-gcp-project-id]"  # <-- REPLACE THIS
GCP_LOCATION = "us-central1"         # Region for Vertex AI and BQ connection
BQ_LOCATION = "US"                   # Location for BigQuery dataset
BQ_DATASET_ID = "mlb_rag_data_2024"  # Dataset to store RAG documents
BQ_RAG_TABLE_ID = "rag_documents"    # Table for documents, embeddings, metadata
BQ_INDEX_NAME = "rag_docs_embedding_idx" # Name for the BQ vector index

# Vertex AI Models
VERTEX_LLM_MODEL = "gemini-1.5-flash-001" # Or gemini-1.0-pro, etc.
VERTEX_EMB_MODEL = "text-embedding-005"
EMBEDDING_TASK_TYPE = "RETRIEVAL_DOCUMENT" # Task type for embeddings
EMBEDDING_DIMENSIONALITY = 768 # Dimension for text-embedding-005

# MLB API Rate Limiting
MLB_API_CALLS = 9 # Be conservative, official limit is 10/min
MLB_API_RATE_LIMIT = 60 # seconds

# Vertex AI API Quotas (Adjust based on your project's quotas)
# Find quotas in GCP Console -> IAM & Admin -> Quotas -> Filter by Vertex AI API
VERTEX_LLM_RPM = 180 # Requests per minute (example for gemini-1.5-flash)
VERTEX_EMB_RPM = 1400 # Requests per minute (example for text-embedding-005)

# How many recent games to process per team
NUM_GAMES_PER_TEAM = 1 # Set low for testing, increase for production

# Team configurations (shortened for brevity)
TEAMS = {
    'rangers': 140,
    # Add all other teams...
    'dodgers': 119,
}

# --- BigQuery Client and Vertex AI Initialization ---
try:
    bq_client = bigquery.Client(project=GCP_PROJECT_ID)
    vertexai.init(project=GCP_PROJECT_ID, location=GCP_LOCATION)
    # Load models (do this once)
    llm_model = GenerativeModel(VERTEX_LLM_MODEL)
    emb_model = TextEmbeddingModel.from_pretrained(VERTEX_EMB_MODEL)
    logger.info(f"Initialized BigQuery client and Vertex AI SDK for project {GCP_PROJECT_ID}")
except Exception as e:
    logger.error(f"Failed to initialize Google Cloud clients: {e}")
    exit() # Critical failure

# --- RAG Table Schema ---
RAG_SCHEMA = [
    bigquery.SchemaField("doc_id", "STRING", mode="REQUIRED", description="Unique ID for the document (e.g., game_pk_summary, game_pk_play_idx)"),
    bigquery.SchemaField("game_id", "INTEGER", mode="REQUIRED", description="MLB gamePk"),
    bigquery.SchemaField("doc_type", "STRING", mode="NULLABLE", description="Type of document (e.g., 'game_summary', 'play_detail')"),
    bigquery.SchemaField("content", "STRING", mode="NULLABLE", description="Natural language text content generated by LLM"),
    bigquery.SchemaField("embedding", "FLOAT64", mode="REPEATED", description=f"Vector embedding ({EMBEDDING_DIMENSIONALITY} dimensions)"),
    bigquery.SchemaField("metadata", "JSON", mode="NULLABLE", description="Structured metadata associated with the content"),
    bigquery.SchemaField("last_updated", "TIMESTAMP", mode="NULLABLE"),
]

# --- Utility Functions ---
@sleep_and_retry
@limits(calls=MLB_API_CALLS, period=MLB_API_RATE_LIMIT)
def call_mlb_api(url: str) -> Dict:
    """Make a rate-limited call to the MLB API"""
    try:
        response = requests.get(url, timeout=20) # Add timeout
        response.raise_for_status()
        # Check if response is valid JSON
        if 'application/json' in response.headers.get('Content-Type', ''):
            return response.json()
        else:
            logger.warning(f"Non-JSON response received from MLB API: {url}. Status: {response.status_code}. Content: {response.text[:100]}...")
            return {} # Return empty dict to signal issue
    except requests.exceptions.RequestException as e:
        logger.error(f"Error calling MLB API {url}: {e}")
        return {} # Return empty dict on error

def ensure_dataset_exists(client: bigquery.Client, dataset_id: str):
    """Create BQ dataset if it doesn't exist"""
    full_dataset_id = f"{client.project}.{dataset_id}"
    try:
        client.get_dataset(full_dataset_id)
        logger.info(f"Dataset {full_dataset_id} already exists")
    except NotFound:
        dataset = bigquery.Dataset(full_dataset_id)
        dataset.location = BQ_LOCATION
        dataset = client.create_dataset(dataset, timeout=30)
        logger.info(f"Created dataset {full_dataset_id}")
    except Exception as e:
        logger.error(f"Error checking/creating dataset {full_dataset_id}: {e}")
        raise

def create_rag_table(client: bigquery.Client, dataset_id: str, table_id: str):
    """Create the RAG table in BigQuery if it doesn't exist."""
    full_table_id = f"{client.project}.{dataset_id}.{table_id}"
    try:
        client.get_table(full_table_id)
        logger.info(f"Table {full_table_id} already exists.")
    except NotFound:
        logger.info(f"Table {full_table_id} not found, creating...")
        table = bigquery.Table(full_table_id, schema=RAG_SCHEMA)
        try:
            client.create_table(table)
            logger.info(f"Created table {full_table_id}")
        except Exception as e:
            logger.error(f"Failed to create table {full_table_id}: {e}")
            raise
    except Exception as e:
         logger.error(f"Error checking table {full_table_id}: {e}")
         raise

# Rate limiter for Vertex AI LLM calls
@sleep_and_retry
@limits(calls=VERTEX_LLM_RPM, period=60)
def call_vertex_llm(prompt: str) -> Optional[str]:
    """Make a rate-limited call to the Vertex AI LLM."""
    try:
        # Configure Gemini generation
        generation_config = GenerationConfig(
            temperature=0.2, # More factual
            max_output_tokens=1024
        )
        response = llm_model.generate_content(
            prompt,
            generation_config=generation_config,
            # safety_settings=... # Add safety settings if needed
            )
        # Basic check for response content
        if response.candidates and response.candidates[0].content.parts:
             # Handle potential lack of text part (though unlikely for text prompts)
            text_parts = [part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')]
            if text_parts:
                return "\n".join(text_parts).strip()
            else:
                 logger.warning(f"LLM response received but no text part found for prompt: {prompt[:100]}...")
                 return None
        else:
            # Log the blocking reason if available
            block_reason = response.prompt_feedback.block_reason if response.prompt_feedback else 'Unknown'
            logger.warning(f"LLM generation blocked or empty. Reason: {block_reason}. Prompt: {prompt[:100]}...")
            return None
    except Exception as e:
        logger.error(f"Error calling Vertex AI LLM: {e}. Prompt: {prompt[:100]}...")
        return None

# Rate limiter for Vertex AI Embedding calls
# Note: The SDK handles batching internally to some extent, but explicit batching + rate limiting is safer for large loads
@sleep_and_retry
@limits(calls=VERTEX_EMB_RPM, period=60)
def call_vertex_embedding(text_inputs: List[str]) -> List[Optional[List[float]]]:
    """Make a rate-limited call to the Vertex AI Embedding API."""
    results = []
    try:
        # Max 250 inputs per API call for text-embedding-005
        instances = [TextEmbeddingInput(text=text, task_type=EMBEDDING_TASK_TYPE) for text in text_inputs]
        kwargs = {"output_dimensionality": EMBEDDING_DIMENSIONALITY} if EMBEDDING_TASK_TYPE != "RETRIEVAL_DOCUMENT" else {}

        embeddings = emb_model.get_embeddings(instances, **kwargs)
        results = [emb.values for emb in embeddings]
        # Basic validation
        if not all(len(emb) == EMBEDDING_DIMENSIONALITY for emb in results if emb):
             logger.warning(f"Generated embeddings have unexpected dimensionality. Expected {EMBEDDING_DIMENSIONALITY}.")
             # Handle or filter out malformed embeddings if needed

        return results # Returns list of lists (embeddings)

    except Exception as e:
        logger.error(f"Error calling Vertex AI Embedding API for batch size {len(text_inputs)}: {e}")
        # Return None for each input in the failed batch
        return [None] * len(text_inputs)


def upload_to_bigquery(df: pd.DataFrame, table_id: str, schema: List[bigquery.SchemaField]):
    """Upload DataFrame to BigQuery table, handling common errors."""
    if df.empty:
        logger.info(f"DataFrame is empty, skipping upload to {table_id}.")
        return

    # Ensure data types match schema, especially JSON
    if 'metadata' in df.columns:
         # Ensure it's a string representation of JSON for BQ JSON type
        df['metadata'] = df['metadata'].apply(lambda x: json.dumps(x) if x and not isinstance(x, str) else x)

    # Ensure timestamp is correct type
    if 'last_updated' in df.columns:
         df['last_updated'] = pd.to_datetime(df['last_updated'], utc=True)

    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_APPEND", # Append new documents
        schema=schema,
        # Specify source format if needed, default is CSV for DataFrames but PARQUET might be better for nested data if converting first
        # source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON, # If metadata causes issues
    )

    try:
        job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_config)
        job.result()  # Wait for the job to complete
        logger.info(f"Loaded {len(df)} rows to {table_id}. Job ID: {job.job_id}")

    except BadRequest as e:
        logger.error(f"BigQuery BadRequest loading to {table_id}: {e}")
        # Log detailed errors
        if hasattr(e, 'errors'):
            for error in e.errors:
                logger.error(f"  Reason: {error.get('reason', 'N/A')}, Location: {error.get('location', 'N/A')}, Message: {error.get('message', 'N/A')}")
        # Consider saving the failed DataFrame for inspection
        # df.to_csv(f"failed_upload_{table_id.split('.')[-1]}_{datetime.now(UTC).strftime('%Y%m%d%H%M%S')}.csv", index=False)
        # raise # Optionally re-raise to stop processing

    except Exception as e:
        logger.error(f"Generic error loading data to {table_id}: {e}")
        # raise # Optionally re-raise

# --- Core Processing Functions ---

def get_recent_game_ids(team_id: int, season: int = 2024, num_games: int = 10) -> List[int]:
    """Fetch recent game IDs for a specified team and season"""
    url = f'https://statsapi.mlb.com/api/v1/schedule?sportId=1&season={season}&teamId={team_id}&fields=dates,games,gamePk,officialDate,status,detailedState'
    schedule_data = call_mlb_api(url)
    game_ids = []
    if schedule_data and 'dates' in schedule_data:
        all_games = []
        for date_entry in schedule_data.get('dates', []):
            for game in date_entry.get('games', []):
                 # Only consider final games for this RAG pipeline example
                 if game.get('status', {}).get('detailedState') == 'Final':
                    all_games.append({
                        'game_id': game.get('gamePk'),
                        'date': game.get('officialDate')
                    })

        # Sort by date descending and take the most recent N
        all_games.sort(key=lambda x: x['date'], reverse=True)
        game_ids = [game['game_id'] for game in all_games[:num_games] if game['game_id']]

    if not game_ids:
         logger.warning(f"No recent final game IDs found for team {team_id}, season {season}.")

    return game_ids


def get_full_game_data(game_pk: int) -> Dict:
    """Fetch the full live feed data for a specific game."""
    url = f'https://statsapi.mlb.com/api/v1.1/game/{game_pk}/feed/live'
    logger.info(f"Fetching full data for game {game_pk}")
    data = call_mlb_api(url)
    if not data:
        logger.error(f"Failed to fetch or received empty data for game {game_pk}")
    return data

def generate_game_summary_and_metadata(game_data: Dict) -> Optional[Tuple[str, Dict]]:
    """Generates a natural language summary and structured metadata for a game using an LLM."""
    if not game_data or 'gameData' not in game_data or 'liveData' not in game_data:
        logger.warning("generate_game_summary_and_metadata: Invalid or incomplete game_data provided.")
        return None

    try:
        game_info = game_data['gameData']
        live_info = game_data['liveData']
        game_pk = game_info['pk']
        status = game_info['status']['detailedState']

        if status != 'Final':
            logger.info(f"Game {game_pk} is not Final ({status}), skipping summary generation.")
            return None

        home_team = game_info['teams']['home']['name']
        away_team = game_info['teams']['away']['name']
        home_id = game_info['teams']['home']['id']
        away_id = game_info['teams']['away']['id']
        date = game_info['datetime']['officialDate']
        venue = game_info['venue']['name']

        # Extract scores (handle potential missing keys)
        linescore = live_info.get('linescore', {})
        home_score = linescore.get('teams', {}).get('home', {}).get('runs', 0)
        away_score = linescore.get('teams', {}).get('away', {}).get('runs', 0)
        innings_data = linescore.get('innings', [])

        # Basic play info extraction (can be made more detailed)
        key_plays_info = []
        all_plays = live_info.get('plays', {}).get('allPlays', [])
        scoring_plays_indices = live_info.get('plays', {}).get('scoringPlays', [])
        scoring_plays = [all_plays[i] for i in scoring_plays_indices if i < len(all_plays)]

        for play in scoring_plays: # Focus on scoring plays for brevity
             play_result = play.get('result', {})
             play_about = play.get('about', {})
             if play_result and play_about:
                 key_plays_info.append(f"Inning {play_about.get('inning', '?')} ({play_about.get('halfInning', '')}): {play_result.get('description', 'N/A')}")
        key_plays_str = "\n".join(key_plays_info) if key_plays_info else "No specific scoring plays highlighted."

        # Construct metadata
        metadata = {
            "date": date,
            "season": int(date[:4]) if date else None,
            "home_team_id": home_id,
            "home_team_name": home_team,
            "away_team_id": away_id,
            "away_team_name": away_team,
            "home_score": home_score,
            "away_score": away_score,
            "venue_name": venue,
            "status": status,
            "innings": len(innings_data),
            # Add more metadata: winning/losing pitcher (requires parsing decisions), key player stats (requires boxscore parsing)
        }

        # Construct Prompt
        prompt = f"""
        Analyze the following MLB game data and provide a concise summary suitable for answering questions about the game.

        Game Details:
        - Date: {date}
        - Teams: {away_team} (Away) vs. {home_team} (Home)
        - Final Score: Away {away_score}, Home {home_score}
        - Venue: {venue}
        - Number of Innings: {len(innings_data)}

        Key Scoring Plays:
        {key_plays_str}

        Instructions:
        Summarize the game's outcome, mention the final score, the teams involved, and the date. Briefly mention the flow of scoring if possible based on the key plays. Do not list every play. Focus on the overall result and significant scoring events. Keep the summary to 2-4 sentences.
        """

        logger.info(f"Generating summary for game {game_pk}")
        summary = call_vertex_llm(prompt)

        if summary:
            return summary, metadata
        else:
            logger.warning(f"LLM failed to generate summary for game {game_pk}")
            return None

    except KeyError as e:
         logger.error(f"KeyError processing game data for summary generation (game {game_data.get('gameData',{}).get('pk','?')}). Missing key: {e}")
         return None
    except Exception as e:
        logger.error(f"Unexpected error generating summary for game {game_data.get('gameData',{}).get('pk','?')}: {e}")
        return None

def create_bq_vector_index(client: bigquery.Client, dataset_id: str, table_id: str, index_name: str):
    """Creates a vector index on the embedding column if it doesn't exist."""
    full_table_id = f"{client.project}.{dataset_id}.{table_id}"
    index_check_sql = f"""
    SELECT index_name
    FROM `{client.project}.{dataset_id}`.INFORMATION_SCHEMA.VECTOR_INDEXES
    WHERE table_name = '{table_id}' AND index_name = '{index_name}';
    """
    create_index_sql = f"""
    CREATE OR REPLACE VECTOR INDEX `{index_name}`
    ON `{full_table_id}`(embedding)
    OPTIONS(distance_type='COSINE', index_type='IVF');
    """ # IVF is often a good default, consider TREE_AH for lower latency if needed

    try:
        # Check if index exists
        query_job = client.query(index_check_sql)
        results = query_job.result()
        if results.total_rows > 0:
            logger.info(f"Vector index {index_name} already exists on {full_table_id}.")
            return

        # Create index if it doesn't exist
        logger.info(f"Creating vector index {index_name} on {full_table_id}...")
        query_job = client.query(create_index_sql)
        query_job.result() # Wait for creation to start (indexing happens async)
        logger.info(f"Vector index {index_name} creation initiated. Indexing will proceed in the background.")

    except Exception as e:
        logger.error(f"Error checking or creating vector index {index_name}: {e}")

# --- Main Processing Logic ---

def process_game_for_rag(game_pk: int, full_table_id: str):
    """Fetches, summarizes, embeds, and uploads data for a single game."""
    game_data = get_full_game_data(game_pk)
    if not game_data:
        return # Skip if data fetching failed

    summary_result = generate_game_summary_and_metadata(game_data)
    if not summary_result:
        return # Skip if summary generation failed

    summary_text, metadata = summary_result

    # Generate embedding for the summary
    # Note: Batching is more efficient. If processing many games, gather summaries first.
    embedding_list = call_vertex_embedding([summary_text])
    if not embedding_list or embedding_list[0] is None:
        logger.error(f"Failed to generate embedding for game {game_pk}")
        return

    embedding = embedding_list[0]

    # Prepare data for BigQuery
    doc_id = f"game_{game_pk}_summary"
    rag_data = {
        "doc_id": [doc_id],
        "game_id": [game_pk],
        "doc_type": ["game_summary"],
        "content": [summary_text],
        "embedding": [embedding],
        "metadata": [metadata], # BQ expects dict for JSON col load from DF
        "last_updated": [datetime.now(UTC)]
    }
    rag_df = pd.DataFrame(rag_data)

    # Upload to BigQuery
    upload_to_bigquery(rag_df, full_table_id, RAG_SCHEMA)

def main():
    """Main function to process data for all configured teams."""
    start_time = time.time()
    logger.info("Starting MLB RAG data pipeline...")

    dataset_id = BQ_DATASET_ID
    table_id = BQ_RAG_TABLE_ID
    full_table_id = f"{GCP_PROJECT_ID}.{dataset_id}.{table_id}"

    # Ensure Dataset and Table exist before processing teams
    try:
        ensure_dataset_exists(bq_client, dataset_id)
        create_rag_table(bq_client, dataset_id, table_id)
    except Exception as e:
        logger.critical(f"Failed to ensure BQ dataset/table exists. Exiting. Error: {e}")
        return # Stop execution if basic setup fails

    all_game_pks_to_process = set()

    # 1. Gather all unique recent game IDs first
    logger.info("Gathering recent game IDs for all teams...")
    for team_name, team_id in TEAMS.items():
        logger.info(f"Fetching game IDs for {team_name}...")
        try:
            team_game_ids = get_recent_game_ids(team_id, num_games=NUM_GAMES_PER_TEAM)
            if team_game_ids:
                all_game_pks_to_process.update(team_game_ids)
            logger.info(f"Found {len(team_game_ids)} recent game(s) for {team_name}.")
            time.sleep(1) # Small delay between teams
        except Exception as e:
            logger.error(f"Error fetching game IDs for {team_name} (ID: {team_id}): {e}")

    logger.info(f"Total unique recent games to process: {len(all_game_pks_to_process)}")

    # 2. Process each unique game
    processed_count = 0
    for game_pk in all_game_pks_to_process:
        try:
            logger.info(f"--- Processing Game PK: {game_pk} ---")
            process_game_for_rag(game_pk, full_table_id)
            processed_count += 1
            # Add a small delay to avoid hitting lower-level API limits if any exist
            time.sleep(0.5)
        except Exception as e:
            logger.error(f"Critical error processing game {game_pk}: {e}", exc_info=True)
            # Decide whether to continue or stop on critical errors
            # continue

    # 3. Create Vector Index (after all data is loaded)
    logger.info("Data loading complete. Ensuring vector index exists...")
    create_bq_vector_index(bq_client, dataset_id, table_id, BQ_INDEX_NAME)

    end_time = time.time()
    logger.info(f"MLB RAG data pipeline finished. Processed {processed_count} games.")
    logger.info(f"Total execution time: {end_time - start_time:.2f} seconds")

if __name__ == "__main__":
    # --- IMPORTANT SETUP ---
    # 1. Replace '[your-gcp-project-id]' at the top.
    # 2. Ensure you have authenticated (`gcloud auth application-default login`).
    # 3. Ensure necessary APIs are enabled in your GCP project.
    # 4. Ensure necessary Python libraries are installed.
    # 5. Review and adjust rate limits and model names if needed.
    # --- --- --- --- --- ---
    main()